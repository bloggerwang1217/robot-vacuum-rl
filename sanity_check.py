"""
DQN Sanity Check Script
ç”¨æ–¼é©—è­‰ DQN å¯¦ä½œæ˜¯å¦æ­£ç¢ºé‹ä½œ

æ¸¬è©¦é …ç›®ï¼š
1. ç°¡åŒ–ç’°å¢ƒå­¸ç¿’æ¸¬è©¦ï¼ˆå–® robot + å–®å……é›»åº§ï¼‰
2. Q-value åˆç†æ€§æª¢æŸ¥
3. Bellman Equation é©—è­‰
4. Target Network åŒæ­¥ç¢ºèª
5. Gradient Flow æª¢æŸ¥
6. Replay Buffer æ­£ç¢ºæ€§
7. éš¨æ©Ÿ vs è¨“ç·´ Policy æ¯”è¼ƒ
"""

import torch
import torch.nn as nn
import numpy as np
import random
import argparse
from collections import deque
from typing import Dict, List, Tuple
import copy

from dqn import DQN, init_weights
from gym import RobotVacuumGymEnv


class SanityCheckAgent:
    """ç°¡åŒ–ç‰ˆçš„ DQN Agentï¼Œç”¨æ–¼ sanity check"""

    def __init__(self, observation_dim: int, action_dim: int, device: torch.device,
                 gamma: float = 0.99, lr: float = 0.001, batch_size: int = 64):
        self.device = device
        self.observation_dim = observation_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.batch_size = batch_size
        self.epsilon = 0.1

        # Networks
        self.q_net = DQN(action_dim, observation_dim).to(device)
        self.target_net = DQN(action_dim, observation_dim).to(device)
        self.q_net.apply(init_weights)
        self.target_net.load_state_dict(self.q_net.state_dict())

        # Optimizer
        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=lr)

        # Replay buffer
        self.memory = deque(maxlen=10000)

        # Stats
        self.train_count = 0
        self.last_loss = None
        self.last_q_values = None

    def select_action(self, obs: np.ndarray, eval_mode: bool = False) -> int:
        if not eval_mode and random.random() < self.epsilon:
            return random.randint(0, self.action_dim - 1)

        with torch.no_grad():
            obs_t = torch.FloatTensor(obs).unsqueeze(0).to(self.device)
            q_values = self.q_net(obs_t)
            self.last_q_values = q_values.cpu().numpy().flatten()
            return q_values.argmax().item()

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def train_step(self, min_buffer_size: int = 100) -> Dict[str, float]:
        if len(self.memory) < min_buffer_size:
            return {}

        self.train_count += 1

        # Sample batch
        batch = random.sample(self.memory, self.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)

        states = torch.FloatTensor(np.array(states)).to(self.device)
        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        dones = torch.FloatTensor(dones).to(self.device)

        # Current Q-values
        q_values = self.q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)

        # Target Q-values (Double DQN)
        with torch.no_grad():
            next_actions = self.q_net(next_states).argmax(1, keepdim=True)
            next_q_values = self.target_net(next_states).gather(1, next_actions).squeeze(1)
            target_q_values = rewards + self.gamma * next_q_values * (1 - dones)

        # Loss
        loss = nn.MSELoss()(q_values, target_q_values)
        self.last_loss = loss.item()

        # Optimize
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return {
            'loss': loss.item(),
            'q_mean': q_values.mean().item(),
            'q_std': q_values.std().item(),
            'target_q_mean': target_q_values.mean().item(),
        }

    def update_target_network(self):
        self.target_net.load_state_dict(self.q_net.state_dict())


def print_header(title: str):
    print("\n" + "=" * 60)
    print(f"  {title}")
    print("=" * 60)


def print_result(test_name: str, passed: bool, details: str = ""):
    status = "âœ… PASS" if passed else "âŒ FAIL"
    print(f"{status} | {test_name}")
    if details:
        print(f"       {details}")


# =============================================================================
# Test 1: ç°¡åŒ–ç’°å¢ƒå­¸ç¿’æ¸¬è©¦
# =============================================================================
def test_simple_learning(device: torch.device, num_episodes: int = 500) -> bool:
    """
    æ¸¬è©¦ï¼šå–® robot ç’°å¢ƒï¼Œåªæœ‰ä¸€å€‹å……é›»åº§åœ¨ä¸­é–“
    é æœŸï¼šrobot æ‡‰è©²å­¸æœƒèµ°å‘å……é›»åº§ä¸¦åœç•™
    """
    print_header("Test 1: Simple Learning (Single Robot to Charger)")

    # å‰µå»ºç°¡åŒ–ç’°å¢ƒï¼š5x5ï¼Œåªæœ‰ä¸­é–“æœ‰å……é›»åº§ï¼Œåªç”¨ robot_0
    env = RobotVacuumGymEnv(
        n=5,
        initial_energy=100,
        robot_energies=[100, 1, 1, 1],  # å…¶ä»– robot ç›´æ¥æ­»æ‰
        e_move=1,
        e_charge=5,
        e_collision=10,
        e_boundary=50,
        n_steps=100,
        charger_positions=[(2, 2)]  # åªæœ‰ä¸­é–“ä¸€å€‹å……é›»åº§
    )

    obs_dim = env.observation_space.shape[0]
    agent = SanityCheckAgent(obs_dim, 5, device, gamma=0.99, lr=0.001)

    # Training
    episode_rewards = []
    episode_lengths = []

    for ep in range(num_episodes):
        obs, _ = env.reset()
        total_reward = 0
        steps = 0
        done = False

        while not done and steps < 100:
            # åªæ§åˆ¶ robot_0
            action = agent.select_action(obs['robot_0'])
            actions = [action, 4, 4, 4]  # å…¶ä»– robot stay

            next_obs, rewards, terms, truncs, _ = env.step(actions)
            reward = rewards['robot_0']
            terminated = terms['robot_0']
            truncated = truncs['robot_0']

            agent.remember(obs['robot_0'], action, reward, next_obs['robot_0'], terminated)
            agent.train_step()

            obs = next_obs
            total_reward += reward
            steps += 1
            done = terminated or truncated

        episode_rewards.append(total_reward)
        episode_lengths.append(steps)

        # Update target network
        if ep % 10 == 0:
            agent.update_target_network()

        # Decay epsilon
        agent.epsilon = max(0.01, agent.epsilon * 0.995)

        if (ep + 1) % 100 == 0:
            avg_reward = np.mean(episode_rewards[-100:])
            avg_length = np.mean(episode_lengths[-100:])
            print(f"  Episode {ep+1}: Avg Reward = {avg_reward:.2f}, Avg Length = {avg_length:.1f}, Epsilon = {agent.epsilon:.3f}")

    # Evaluate final policy
    agent.epsilon = 0  # Greedy
    eval_rewards = []

    for _ in range(20):
        obs, _ = env.reset()
        total_reward = 0
        steps = 0
        done = False

        while not done and steps < 100:
            action = agent.select_action(obs['robot_0'], eval_mode=True)
            actions = [action, 4, 4, 4]
            next_obs, rewards, terms, truncs, _ = env.step(actions)
            obs = next_obs
            total_reward += rewards['robot_0']
            steps += 1
            done = terms['robot_0'] or truncs['robot_0']

        eval_rewards.append(total_reward)

    avg_eval_reward = np.mean(eval_rewards)

    # Compare with random policy
    random_rewards = []
    for _ in range(20):
        obs, _ = env.reset()
        total_reward = 0
        steps = 0
        done = False

        while not done and steps < 100:
            action = random.randint(0, 4)
            actions = [action, 4, 4, 4]
            next_obs, rewards, terms, truncs, _ = env.step(actions)
            obs = next_obs
            total_reward += rewards['robot_0']
            steps += 1
            done = terms['robot_0'] or truncs['robot_0']

        random_rewards.append(total_reward)

    avg_random_reward = np.mean(random_rewards)

    improvement = avg_eval_reward - avg_random_reward
    passed = improvement > 10  # è‡³å°‘æ¯” random å¥½ 10 åˆ†

    print(f"\n  Final Evaluation:")
    print(f"    Trained Policy Avg Reward: {avg_eval_reward:.2f}")
    print(f"    Random Policy Avg Reward:  {avg_random_reward:.2f}")
    print(f"    Improvement: {improvement:.2f}")

    print_result("Simple Learning", passed,
                f"Improvement = {improvement:.2f} (threshold: > 10)")

    return passed


# =============================================================================
# Test 2: Q-value åˆç†æ€§æª¢æŸ¥
# =============================================================================
def test_q_value_reasonableness(device: torch.device) -> bool:
    """
    æ¸¬è©¦ï¼šQ-value æ˜¯å¦åœ¨åˆç†ç¯„åœå…§
    é æœŸï¼šQ-value æ‡‰è©²åœ¨ç†è«–æœ€å¤§å€¼ç¯„åœå…§ï¼Œä¸æ‡‰è©² NaN æˆ– Inf
    """
    print_header("Test 2: Q-value Reasonableness")

    env = RobotVacuumGymEnv(
        n=3,
        initial_energy=100,
        e_move=1,
        e_charge=5,
        e_collision=3,
        n_steps=500
    )

    obs_dim = env.observation_space.shape[0]
    agent = SanityCheckAgent(obs_dim, 5, device, gamma=0.99)

    # Train for a bit
    for ep in range(100):
        obs, _ = env.reset()
        done = False
        steps = 0

        while not done and steps < 100:
            actions = [agent.select_action(obs['robot_0'])] + [random.randint(0, 4) for _ in range(3)]
            next_obs, rewards, terms, truncs, _ = env.step(actions)
            agent.remember(obs['robot_0'], actions[0], rewards['robot_0'],
                          next_obs['robot_0'], terms['robot_0'])
            agent.train_step(min_buffer_size=50)
            obs = next_obs
            steps += 1
            done = terms['robot_0'] or any(truncs.values())

        if ep % 10 == 0:
            agent.update_target_network()

    # Check Q-values
    obs, _ = env.reset()
    with torch.no_grad():
        obs_t = torch.FloatTensor(obs['robot_0']).unsqueeze(0).to(device)
        q_values = agent.q_net(obs_t).cpu().numpy().flatten()

    # Theoretical max Q-value: r_max / (1 - gamma)
    # å……é›»çå‹µ = 20, èƒ½é‡çå‹µ â‰ˆ 0.25, æ­»äº¡æ‡²ç½° = -100
    # æ¨‚è§€ä¼°è¨ˆæ¯æ­¥ +20 -> Q_max â‰ˆ 20 / 0.01 = 2000
    theoretical_max = 2000

    has_nan = np.any(np.isnan(q_values))
    has_inf = np.any(np.isinf(q_values))
    within_range = np.all(np.abs(q_values) < theoretical_max)

    print(f"  Q-values: {q_values}")
    print(f"  Q-value range: [{q_values.min():.2f}, {q_values.max():.2f}]")
    print(f"  Theoretical max: {theoretical_max}")
    print(f"  Has NaN: {has_nan}")
    print(f"  Has Inf: {has_inf}")
    print(f"  Within theoretical range: {within_range}")

    passed = not has_nan and not has_inf and within_range
    print_result("Q-value Reasonableness", passed)

    return passed


# =============================================================================
# Test 3: Bellman Equation é©—è­‰
# =============================================================================
def test_bellman_equation(device: torch.device) -> bool:
    """
    æ¸¬è©¦ï¼šè¨“ç·´å¾Œçš„ Q-values æ˜¯å¦æ»¿è¶³ Bellman equation
    é æœŸï¼šQ(s,a) â‰ˆ r + Î³ * max_a' Q(s', a') çš„èª¤å·®æ‡‰è©²å¾ˆå°
    """
    print_header("Test 3: Bellman Equation Verification")

    env = RobotVacuumGymEnv(
        n=3,
        initial_energy=100,
        e_move=1,
        e_charge=5,
        e_collision=3,
        n_steps=500
    )

    obs_dim = env.observation_space.shape[0]
    agent = SanityCheckAgent(obs_dim, 5, device, gamma=0.99)

    # Train
    for ep in range(200):
        obs, _ = env.reset()
        done = False
        steps = 0

        while not done and steps < 100:
            actions = [agent.select_action(obs['robot_0'])] + [random.randint(0, 4) for _ in range(3)]
            next_obs, rewards, terms, truncs, _ = env.step(actions)
            agent.remember(obs['robot_0'], actions[0], rewards['robot_0'],
                          next_obs['robot_0'], terms['robot_0'])
            agent.train_step(min_buffer_size=50)
            obs = next_obs
            steps += 1
            done = terms['robot_0'] or any(truncs.values())

        if ep % 10 == 0:
            agent.update_target_network()

    # Verify Bellman on a sample of transitions
    if len(agent.memory) < 100:
        print("  Not enough samples in buffer")
        return False

    sample = random.sample(list(agent.memory), 100)
    bellman_errors = []

    for state, action, reward, next_state, done in sample:
        with torch.no_grad():
            state_t = torch.FloatTensor(state).unsqueeze(0).to(device)
            next_state_t = torch.FloatTensor(next_state).unsqueeze(0).to(device)

            q_value = agent.q_net(state_t)[0, action].item()
            next_q_max = agent.target_net(next_state_t).max().item()

            expected_q = reward + (1 - done) * agent.gamma * next_q_max
            bellman_error = abs(q_value - expected_q)
            bellman_errors.append(bellman_error)

    avg_error = np.mean(bellman_errors)
    max_error = np.max(bellman_errors)

    print(f"  Sample size: {len(sample)}")
    print(f"  Average Bellman Error: {avg_error:.4f}")
    print(f"  Max Bellman Error: {max_error:.4f}")

    # ç¶“éè¨“ç·´å¾Œï¼Œå¹³å‡èª¤å·®æ‡‰è©²ç›¸å°è¼ƒå°
    passed = avg_error < 50  # é€™å€‹é–¾å€¼å–æ±ºæ–¼ reward scale
    print_result("Bellman Equation", passed, f"Avg Error = {avg_error:.4f}")

    return passed


# =============================================================================
# Test 4: Target Network åŒæ­¥ç¢ºèª
# =============================================================================
def test_target_network_sync(device: torch.device) -> bool:
    """
    æ¸¬è©¦ï¼šupdate_target_network() æ˜¯å¦æ­£ç¢ºåŒæ­¥æ¬Šé‡
    """
    print_header("Test 4: Target Network Sync")

    agent = SanityCheckAgent(29, 5, device)

    # è¨“ç·´å¹¾æ­¥è®“ q_net å’Œ target_net ä¸åŒ
    for _ in range(10):
        state = np.random.randn(29).astype(np.float32)
        action = random.randint(0, 4)
        reward = random.random()
        next_state = np.random.randn(29).astype(np.float32)
        done = False
        agent.remember(state, action, reward, next_state, done)

    for _ in range(50):
        agent.train_step(min_buffer_size=5)

    # æª¢æŸ¥åŒæ­¥å‰æ˜¯å¦ä¸åŒ
    diff_before = 0
    for p1, p2 in zip(agent.q_net.parameters(), agent.target_net.parameters()):
        diff_before += (p1 - p2).abs().sum().item()

    print(f"  Parameter difference before sync: {diff_before:.6f}")

    # åŒæ­¥
    agent.update_target_network()

    # æª¢æŸ¥åŒæ­¥å¾Œæ˜¯å¦ç›¸åŒ
    diff_after = 0
    for p1, p2 in zip(agent.q_net.parameters(), agent.target_net.parameters()):
        diff_after += (p1 - p2).abs().sum().item()

    print(f"  Parameter difference after sync: {diff_after:.6f}")

    passed = diff_after < 1e-6 and diff_before > 0
    print_result("Target Network Sync", passed)

    return passed


# =============================================================================
# Test 5: Gradient Flow æª¢æŸ¥
# =============================================================================
def test_gradient_flow(device: torch.device) -> bool:
    """
    æ¸¬è©¦ï¼šæ¢¯åº¦æ˜¯å¦æ­£ç¢ºæµå‹•åˆ°æ‰€æœ‰å±¤
    """
    print_header("Test 5: Gradient Flow")

    agent = SanityCheckAgent(29, 5, device)

    # å¡«å…… buffer
    for _ in range(100):
        state = np.random.randn(29).astype(np.float32)
        action = random.randint(0, 4)
        reward = random.random() * 10
        next_state = np.random.randn(29).astype(np.float32)
        done = random.random() < 0.1
        agent.remember(state, action, reward, next_state, done)

    # åŸ·è¡Œä¸€æ¬¡ train step
    agent.train_step(min_buffer_size=50)

    # æª¢æŸ¥æ¯å±¤çš„æ¢¯åº¦
    grad_info = []
    all_have_grad = True

    for name, param in agent.q_net.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            grad_info.append((name, grad_norm))
            if grad_norm == 0:
                all_have_grad = False
        else:
            grad_info.append((name, None))
            all_have_grad = False

    print("  Layer gradients:")
    for name, grad_norm in grad_info:
        if grad_norm is not None:
            status = "âœ“" if grad_norm > 0 else "âœ— (zero)"
            print(f"    {status} {name}: {grad_norm:.6f}")
        else:
            print(f"    âœ— {name}: None")

    print_result("Gradient Flow", all_have_grad)

    return all_have_grad


# =============================================================================
# Test 6: Replay Buffer æ­£ç¢ºæ€§
# =============================================================================
def test_replay_buffer(device: torch.device) -> bool:
    """
    æ¸¬è©¦ï¼šReplay buffer æ˜¯å¦æ­£ç¢ºå­˜å…¥å’Œå–å‡º transitions
    """
    print_header("Test 6: Replay Buffer Correctness")

    agent = SanityCheckAgent(29, 5, device)

    # å­˜å…¥å·²çŸ¥çš„ transitions
    test_transitions = []
    for i in range(10):
        state = np.full(29, i, dtype=np.float32)
        action = i % 5
        reward = float(i)
        next_state = np.full(29, i + 1, dtype=np.float32)
        done = (i == 9)

        test_transitions.append((state.copy(), action, reward, next_state.copy(), done))
        agent.remember(state, action, reward, next_state, done)

    # é©—è­‰ buffer å…§å®¹
    all_correct = True

    for i, (stored, original) in enumerate(zip(list(agent.memory), test_transitions)):
        state_match = np.allclose(stored[0], original[0])
        action_match = stored[1] == original[1]
        reward_match = stored[2] == original[2]
        next_state_match = np.allclose(stored[3], original[3])
        done_match = stored[4] == original[4]

        if not all([state_match, action_match, reward_match, next_state_match, done_match]):
            print(f"  Mismatch at index {i}")
            all_correct = False

    # æ¸¬è©¦ maxlen
    agent2 = SanityCheckAgent(29, 5, device)
    agent2.memory = deque(maxlen=5)

    for i in range(10):
        state = np.full(29, i, dtype=np.float32)
        agent2.remember(state, i % 5, float(i), state, False)

    buffer_len_correct = len(agent2.memory) == 5
    oldest_removed = agent2.memory[0][2] == 5.0  # oldest should be reward=5, not 0

    print(f"  All transitions stored correctly: {all_correct}")
    print(f"  Buffer maxlen works: {buffer_len_correct}")
    print(f"  Oldest removed correctly: {oldest_removed}")

    passed = all_correct and buffer_len_correct and oldest_removed
    print_result("Replay Buffer", passed)

    return passed


# =============================================================================
# Test 7: éš¨æ©Ÿ vs è¨“ç·´ Policy æ¯”è¼ƒ
# =============================================================================
def test_policy_improvement(device: torch.device, num_episodes: int = 300) -> bool:
    """
    æ¸¬è©¦ï¼šè¨“ç·´å¾Œçš„ policy æ˜¯å¦æ¯”éš¨æ©Ÿ policy å¥½
    """
    print_header("Test 7: Policy Improvement over Random")

    env = RobotVacuumGymEnv(
        n=3,
        initial_energy=100,
        e_move=1,
        e_charge=5,
        e_collision=3,
        n_steps=200
    )

    obs_dim = env.observation_space.shape[0]
    agent = SanityCheckAgent(obs_dim, 5, device, gamma=0.99, lr=0.001)

    # Baseline: Random policy
    print("  Evaluating random policy...")
    random_rewards = []
    for _ in range(30):
        obs, _ = env.reset()
        total_reward = 0
        done = False
        steps = 0

        while not done and steps < 200:
            actions = [random.randint(0, 4) for _ in range(4)]
            next_obs, rewards, terms, truncs, _ = env.step(actions)
            total_reward += rewards['robot_0']
            obs = next_obs
            steps += 1
            done = terms['robot_0'] or any(truncs.values())

        random_rewards.append(total_reward)

    avg_random = np.mean(random_rewards)
    print(f"    Random Policy: {avg_random:.2f} Â± {np.std(random_rewards):.2f}")

    # Train
    print(f"  Training for {num_episodes} episodes...")
    for ep in range(num_episodes):
        obs, _ = env.reset()
        done = False
        steps = 0

        while not done and steps < 200:
            action = agent.select_action(obs['robot_0'])
            actions = [action] + [random.randint(0, 4) for _ in range(3)]
            next_obs, rewards, terms, truncs, _ = env.step(actions)
            agent.remember(obs['robot_0'], action, rewards['robot_0'],
                          next_obs['robot_0'], terms['robot_0'])
            agent.train_step(min_buffer_size=100)
            obs = next_obs
            steps += 1
            done = terms['robot_0'] or any(truncs.values())

        if ep % 10 == 0:
            agent.update_target_network()
        agent.epsilon = max(0.01, agent.epsilon * 0.99)

    # Evaluate trained policy
    print("  Evaluating trained policy...")
    agent.epsilon = 0
    trained_rewards = []

    for _ in range(30):
        obs, _ = env.reset()
        total_reward = 0
        done = False
        steps = 0

        while not done and steps < 200:
            action = agent.select_action(obs['robot_0'], eval_mode=True)
            actions = [action] + [random.randint(0, 4) for _ in range(3)]
            next_obs, rewards, terms, truncs, _ = env.step(actions)
            total_reward += rewards['robot_0']
            obs = next_obs
            steps += 1
            done = terms['robot_0'] or any(truncs.values())

        trained_rewards.append(total_reward)

    avg_trained = np.mean(trained_rewards)
    print(f"    Trained Policy: {avg_trained:.2f} Â± {np.std(trained_rewards):.2f}")

    improvement = avg_trained - avg_random
    improvement_pct = (improvement / abs(avg_random)) * 100 if avg_random != 0 else 0

    print(f"\n  Improvement: {improvement:.2f} ({improvement_pct:.1f}%)")

    passed = improvement > 5  # è‡³å°‘å¥½ 5 åˆ†
    print_result("Policy Improvement", passed,
                f"Trained ({avg_trained:.2f}) vs Random ({avg_random:.2f})")

    return passed


# =============================================================================
# Test 8: Loss ä¸‹é™æª¢æŸ¥
# =============================================================================
def test_loss_decreasing(device: torch.device) -> bool:
    """
    æ¸¬è©¦ï¼šLoss æ˜¯å¦éš¨è¨“ç·´ä¸‹é™
    """
    print_header("Test 8: Loss Decreasing")

    env = RobotVacuumGymEnv(
        n=3,
        initial_energy=100,
        e_move=1,
        e_charge=5,
        e_collision=3,
        n_steps=200
    )

    obs_dim = env.observation_space.shape[0]
    agent = SanityCheckAgent(obs_dim, 5, device, gamma=0.99, lr=0.001, batch_size=32)

    # Collect some data first
    print("  Collecting initial data...")
    for _ in range(20):
        obs, _ = env.reset()
        done = False
        steps = 0

        while not done and steps < 100:
            actions = [random.randint(0, 4) for _ in range(4)]
            next_obs, rewards, terms, truncs, _ = env.step(actions)
            agent.remember(obs['robot_0'], actions[0], rewards['robot_0'],
                          next_obs['robot_0'], terms['robot_0'])
            obs = next_obs
            steps += 1
            done = any(terms.values()) or any(truncs.values())

    # Train and record loss
    print("  Training and recording loss...")
    losses = []

    for i in range(500):
        stats = agent.train_step(min_buffer_size=100)
        if stats:
            losses.append(stats['loss'])

        # Occasionally add new data
        if i % 50 == 0:
            obs, _ = env.reset()
            for _ in range(20):
                actions = [agent.select_action(obs['robot_0'])] + [random.randint(0, 4) for _ in range(3)]
                next_obs, rewards, terms, truncs, _ = env.step(actions)
                agent.remember(obs['robot_0'], actions[0], rewards['robot_0'],
                              next_obs['robot_0'], terms['robot_0'])
                obs = next_obs
                if any(terms.values()) or any(truncs.values()):
                    break
            agent.update_target_network()

    if len(losses) < 100:
        print("  Not enough loss samples")
        return False

    # Compare early vs late losses
    early_loss = np.mean(losses[:50])
    late_loss = np.mean(losses[-50:])

    print(f"  Early loss (first 50): {early_loss:.4f}")
    print(f"  Late loss (last 50): {late_loss:.4f}")
    print(f"  Loss reduction: {early_loss - late_loss:.4f}")

    # Loss æ‡‰è©²ä¸‹é™ï¼Œä½†ä¸ä¸€å®šå–®èª¿
    passed = late_loss < early_loss * 1.5  # å…è¨±ä¸€äº›æ³¢å‹•
    print_result("Loss Decreasing", passed,
                f"Early={early_loss:.4f}, Late={late_loss:.4f}")

    return passed


# =============================================================================
# Main
# =============================================================================
def main():
    parser = argparse.ArgumentParser(description="DQN Sanity Check")
    parser.add_argument("--device", type=str, default="auto",
                       help="Device to use (auto/cpu/cuda/mps)")
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    parser.add_argument("--quick", action="store_true",
                       help="Run quick tests only (skip long training tests)")
    args = parser.parse_args()

    # Set seed
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)

    # Device
    if args.device == "auto":
        if torch.cuda.is_available():
            device = torch.device("cuda")
        elif torch.backends.mps.is_available():
            device = torch.device("mps")
        else:
            device = torch.device("cpu")
    else:
        device = torch.device(args.device)

    print("\n" + "=" * 60)
    print("  DQN SANITY CHECK")
    print("=" * 60)
    print(f"Device: {device}")
    print(f"Seed: {args.seed}")
    print(f"Quick mode: {args.quick}")

    results = {}

    # Quick tests (always run)
    results['target_sync'] = test_target_network_sync(device)
    results['gradient_flow'] = test_gradient_flow(device)
    results['replay_buffer'] = test_replay_buffer(device)
    results['q_value'] = test_q_value_reasonableness(device)
    results['bellman'] = test_bellman_equation(device)

    # Longer tests (skip if quick mode)
    if not args.quick:
        results['loss_decreasing'] = test_loss_decreasing(device)
        results['simple_learning'] = test_simple_learning(device, num_episodes=300)
        results['policy_improvement'] = test_policy_improvement(device, num_episodes=200)
    else:
        print("\n  (Skipping long training tests in quick mode)")

    # Summary
    print_header("SUMMARY")

    passed_count = sum(results.values())
    total_count = len(results)

    for test_name, passed in results.items():
        status = "âœ…" if passed else "âŒ"
        print(f"  {status} {test_name}")

    print(f"\n  Total: {passed_count}/{total_count} tests passed")

    if passed_count == total_count:
        print("\n  ğŸ‰ All sanity checks PASSED!")
    else:
        print("\n  âš ï¸  Some tests FAILED. Please investigate.")

    return passed_count == total_count


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
